{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhruthick/cse256project/blob/main/models/bert-base-256-5eps-0.94.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUBwjgZg_PRi"
      },
      "source": [
        "# Transformer Trial one - BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9yhxSC7_O5g"
      },
      "source": [
        "## Imports and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cU3E-L0Y-_pB",
        "outputId": "97b613ae-c59c-45ba-ee0a-f320b7818708"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMa7MRrPA8Ha",
        "outputId": "4dd8a4d1-bf39-4c36-e8af-a57b6915ac33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.29.2\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3ymtK9q--y3Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e48fd7a7-7b26-483c-9cbf-e59a1f28e2b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found device: Tesla T4, n_gpu: 1\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Confirm that the GPU is detected\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  # Get the GPU device name.\n",
        "  device_name = torch.cuda.get_device_name()\n",
        "  n_gpu = torch.cuda.device_count()\n",
        "  print(f\"Found device: {device_name}, n_gpu: {n_gpu}\")\n",
        "  device = torch.device(\"cuda\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ujCEa6rI_A8o"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjQ-N8JrAvHI"
      },
      "source": [
        "## Read Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "AFjtbOwu_bu7"
      },
      "outputs": [],
      "source": [
        "train=pd.read_csv('/content/drive/MyDrive/cse256/project/data/train.csv')\n",
        "val=pd.read_csv('/content/drive/MyDrive/cse256/project/data/validation.csv')\n",
        "test=pd.read_csv('/content/drive/MyDrive/cse256/project/data/test.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61iUMKA3Aw26"
      },
      "source": [
        "## Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "ox3j0IvEAU2d"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_format(sentences):\n",
        "  tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "  # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "  input_ids = []\n",
        "  attention_masks = []\n",
        "\n",
        "  # For every sentence...\n",
        "  for sentence in sentences:\n",
        "      # `encode_plus` will:\n",
        "      #   (1) Tokenize the sentence.\n",
        "      #   (2) Prepend the `[CLS]` token to the start.\n",
        "      #   (3) Append the `[SEP]` token to the end.\n",
        "      #   (4) Map tokens to their IDs.\n",
        "      #   (5) Pad or truncate the sentence to `max_length`\n",
        "      #   (6) Create attention masks for [PAD] tokens.\n",
        "      encoded_dict = tokenizer.encode_plus(\n",
        "                          sentence,                      # Sentence to encode.\n",
        "                          add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                          max_length = 256,           # Pad & truncate all sentences.\n",
        "                          padding = 'max_length',\n",
        "                          truncation = True,\n",
        "                          return_attention_mask = True,   # Construct attn. masks.\n",
        "                          return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                    )\n",
        "\n",
        "      # Add the encoded sentence to the list.\n",
        "      input_ids.append(encoded_dict['input_ids'])\n",
        "\n",
        "      # And its attention mask (simply differentiates padding from non-padding).\n",
        "      attention_masks.append(encoded_dict['attention_mask'])\n",
        "  return input_ids, attention_masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "LgPl1L2w_yMc"
      },
      "outputs": [],
      "source": [
        "def get_input_and_labels(df):\n",
        "  input_ids, attention_masks = tokenize_and_format(df.lyrics.values)\n",
        "  input_ids = torch.cat(input_ids, dim=0)\n",
        "  attention_masks = torch.cat(attention_masks, dim=0)\n",
        "  labels = torch.tensor(df.mood_encoded.values)\n",
        "  return input_ids,attention_masks,labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "_S83FXTTNVTh"
      },
      "outputs": [],
      "source": [
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "cPuEjmMmGerf"
      },
      "outputs": [],
      "source": [
        "# function to get validation accuracy\n",
        "def get_validation_performance(val_set):\n",
        "    # Put the model in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "\n",
        "    num_batches = int(len(val_set)/batch_size) + 1\n",
        "\n",
        "    total_correct = 0\n",
        "\n",
        "    for i in range(num_batches):\n",
        "\n",
        "      end_index = min(batch_size * (i+1), len(val_set))\n",
        "\n",
        "      batch = val_set[i*batch_size:end_index]\n",
        "      \n",
        "      if len(batch) == 0: continue\n",
        "\n",
        "      input_id_tensors = torch.stack([data[0] for data in batch])\n",
        "      input_mask_tensors = torch.stack([data[1] for data in batch])\n",
        "      label_tensors = torch.stack([data[2] for data in batch])\n",
        "      \n",
        "      # Move tensors to the GPU\n",
        "      b_input_ids = input_id_tensors.to(device)\n",
        "      b_input_mask = input_mask_tensors.to(device)\n",
        "      b_labels = label_tensors.to(device)\n",
        "        \n",
        "      # Tell pytorch not to bother with constructing the compute graph during\n",
        "      # the forward pass, since this is only needed for backprop (training).\n",
        "      with torch.no_grad():        \n",
        "\n",
        "        # Forward pass, calculate logit predictions.\n",
        "        outputs = model(b_input_ids, \n",
        "                                token_type_ids=None, \n",
        "                                attention_mask=b_input_mask,\n",
        "                                labels=b_labels)\n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "            \n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += loss.item()\n",
        "        \n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the number of correctly labeled examples in batch\n",
        "        pred_flat = np.argmax(logits, axis=1).flatten()\n",
        "        labels_flat = label_ids.flatten()\n",
        "        num_correct = np.sum(pred_flat == labels_flat)\n",
        "        total_correct += num_correct\n",
        "        \n",
        "    # Report the final accuracy for this validation run.\n",
        "    avg_val_accuracy = total_correct / len(val_set)\n",
        "    return avg_val_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnyUijYxEao5"
      },
      "source": [
        "## Pre-process Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bl0PrH34Edwc"
      },
      "source": [
        "Get Input IDs, attention masks, and labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "OaNI5-ccDv6C"
      },
      "outputs": [],
      "source": [
        "X_train_iids,X_train_ams,y_train=get_input_and_labels(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "OBb-xmXQGiPd"
      },
      "outputs": [],
      "source": [
        "X_val_iids,X_val_ams,y_val=get_input_and_labels(val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "dIRDQtHRLusY"
      },
      "outputs": [],
      "source": [
        "train_set = [(X_train_iids[i], X_train_ams[i], y_train[i]) for i in range(len(y_train))]\n",
        "val_set = [(X_val_iids[i], X_val_ams[i], y_val[i]) for i in range(len(y_val))]\n",
        "\n",
        "train_text = [train.lyrics.values[i] for i in range(len(y_train))]\n",
        "val_text = [val.lyrics.values[i] for i in range(len(y_val))]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdHOhyi7F61p"
      },
      "source": [
        "# Intialize model and train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ExtS8chF1YV",
        "outputId": "6dd3b9b7-e37b-4399-d8c0-eaa61036cc0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 4, # The number of output labels.   \n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "701qz9AJF--A"
      },
      "outputs": [],
      "source": [
        "batch_size = 16\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 3e-5, # args.learning_rate - default is 5e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8\n",
        "                )\n",
        "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, verbose=True, gamma=0.1)\n",
        "epochs = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75exxItNG6Kg",
        "outputId": "e5b0a7a9-5001-4063-ec04-6ee7c1acca0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 5 ========\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 127/127 [01:30<00:00,  1.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total loss: 107.9530328810215\n",
            "Train accuracy: 0.9266600594648167\n",
            "Validation accuracy: 0.9051383399209486\n",
            "\n",
            "======== Epoch 2 / 5 ========\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 127/127 [01:29<00:00,  1.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total loss: 32.995470348745584\n",
            "Train accuracy: 0.9816650148662042\n",
            "Validation accuracy: 0.9407114624505929\n",
            "\n",
            "======== Epoch 3 / 5 ========\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 127/127 [01:29<00:00,  1.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total loss: 15.534122405573726\n",
            "Train accuracy: 0.9905847373637264\n",
            "Validation accuracy: 0.9288537549407114\n",
            "\n",
            "======== Epoch 4 / 5 ========\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 127/127 [01:29<00:00,  1.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total loss: 8.130290664732456\n",
            "Train accuracy: 0.987611496531219\n",
            "Validation accuracy: 0.932806324110672\n",
            "\n",
            "======== Epoch 5 / 5 ========\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 127/127 [01:29<00:00,  1.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total loss: 4.322131160181016\n",
            "Train accuracy: 0.998017839444995\n",
            "Validation accuracy: 0.9407114624505929\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "# training loop\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Put the model into training mode.\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    num_batches = int(len(train_set)/batch_size) + 1\n",
        "\n",
        "    for i in tqdm(range(num_batches)):\n",
        "      end_index = min(batch_size * (i+1), len(train_set))\n",
        "\n",
        "      batch = train_set[i*batch_size:end_index]\n",
        "\n",
        "      if len(batch) == 0: continue\n",
        "\n",
        "      input_id_tensors = torch.stack([data[0] for data in batch])\n",
        "      input_mask_tensors = torch.stack([data[1] for data in batch])\n",
        "      label_tensors = torch.stack([data[2] for data in batch])\n",
        "\n",
        "      # Move tensors to the GPU\n",
        "      b_input_ids = input_id_tensors.to(device)\n",
        "      b_input_mask = input_mask_tensors.to(device)\n",
        "      b_labels = label_tensors.to(device)\n",
        "\n",
        "      # Clear the previously calculated gradient\n",
        "      model.zero_grad()        \n",
        "\n",
        "      # Perform a forward pass (evaluate the model on this training batch).\n",
        "      outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask, \n",
        "                            labels=b_labels)\n",
        "      loss = outputs.loss\n",
        "      logits = outputs.logits\n",
        "\n",
        "      total_train_loss += loss.item()\n",
        "\n",
        "      # Perform a backward pass to calculate the gradients.\n",
        "      loss.backward()\n",
        "\n",
        "      # Update parameters and take a step using the computed gradient.\n",
        "      optimizer.step()\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set. Implement this function in the cell above.\n",
        "    print(f\"Total loss: {total_train_loss}\")\n",
        "    train_acc = get_validation_performance(train_set)\n",
        "    print(f\"Train accuracy: {train_acc}\")\n",
        "    val_acc = get_validation_performance(val_set)\n",
        "    print(f\"Validation accuracy: {val_acc}\")\n",
        "    # scheduler.step()\n",
        "    \n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "g7GcnpRMNpWU"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMNuavGBgz5AT8AuKSER/lZ",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}